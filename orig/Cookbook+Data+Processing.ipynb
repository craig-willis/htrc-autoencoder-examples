{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cookbook Processing\n",
    "\n",
    "This notebook outlines the data collection and preparation for a set of cookbooks in the HathiTrust Digital Library. Analysis is in the other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader, utils\n",
    "from htrc import workset\n",
    "import pandas as pd\n",
    "import os\n",
    "from dask.delayed import delayed\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "import spacy\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Of a 2016 volume workset, 1781 have feature files'"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection = workset.load_hathitrust_collection('https://babel.hathitrust.org/cgi/mb?a=listis&c=494231066')[1:] # First work is a dud\n",
    "paths = ['../features/'+utils.id_to_rsync(id) for id in collection]\n",
    "paths = [path for path in paths if os.path.exists(path)]\n",
    "\"Of a %d volume workset, %d have feature files\" % (len(collection), len(paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 12.9 µs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rights_attributes</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>classification</th>\n",
       "      <th>author</th>\n",
       "      <th>pub_place</th>\n",
       "      <th>page_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>uc1.31822031039282</td>\n",
       "      <td>pd</td>\n",
       "      <td>Natural food recipes  / Mrs. B. Stanford Claunch</td>\n",
       "      <td>1939</td>\n",
       "      <td>TX837 .C5</td>\n",
       "      <td>Claunch, B. Stanford Mrs</td>\n",
       "      <td>cau</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>uc1.31822031043284</td>\n",
       "      <td>ic</td>\n",
       "      <td>Cesar salads / by Judy Hogness ; graphics: R. ...</td>\n",
       "      <td>1977</td>\n",
       "      <td></td>\n",
       "      <td>Hogness, Judy</td>\n",
       "      <td>cau</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1169</th>\n",
       "      <td>uc1.31822031042138</td>\n",
       "      <td>pd</td>\n",
       "      <td>Namco brand crab, the world's finest</td>\n",
       "      <td>1927</td>\n",
       "      <td></td>\n",
       "      <td>Bradley, Alice 1875-1946</td>\n",
       "      <td>cau</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>967</th>\n",
       "      <td>uc1.31822031039365</td>\n",
       "      <td>pd</td>\n",
       "      <td>Kampkookery and useful hints for the motor camper</td>\n",
       "      <td>1927</td>\n",
       "      <td></td>\n",
       "      <td>American Gas Machine Company</td>\n",
       "      <td>mnu</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>uc1.31822031043268</td>\n",
       "      <td>ic</td>\n",
       "      <td>The Sonoma Dried Tomato cookbook, or, What to ...</td>\n",
       "      <td>1992</td>\n",
       "      <td></td>\n",
       "      <td>Waltenspiel, Ronald</td>\n",
       "      <td>cau</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id rights_attributes  \\\n",
       "1174  uc1.31822031039282                pd   \n",
       "280   uc1.31822031043284                ic   \n",
       "1169  uc1.31822031042138                pd   \n",
       "967   uc1.31822031039365                pd   \n",
       "1519  uc1.31822031043268                ic   \n",
       "\n",
       "                                                  title  year classification  \\\n",
       "1174   Natural food recipes  / Mrs. B. Stanford Claunch  1939      TX837 .C5   \n",
       "280   Cesar salads / by Judy Hogness ; graphics: R. ...  1977                  \n",
       "1169               Namco brand crab, the world's finest  1927                  \n",
       "967   Kampkookery and useful hints for the motor camper  1927                  \n",
       "1519  The Sonoma Dried Tomato cookbook, or, What to ...  1992                  \n",
       "\n",
       "                             author pub_place  page_count  \n",
       "1174      Claunch, B. Stanford Mrs        cau          52  \n",
       "280                  Hogness, Judy        cau          84  \n",
       "1169      Bradley, Alice 1875-1946        cau          16  \n",
       "967   American Gas Machine Company        mnu          36  \n",
       "1519           Waltenspiel, Ronald        cau         156  "
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_attrs = ['id', 'rights_attributes', 'title', 'year', 'classification', 'author', 'pub_place', 'page_count']\n",
    "metas = []\n",
    "fr = FeatureReader(paths)\n",
    "for vol in fr.volumes():\n",
    "    meta = pd.Series([getattr(vol, attr) for attr in meta_attrs], index=meta_attrs)\n",
    "    metas.append(meta)\n",
    "all_meta = pd.DataFrame(metas)\n",
    "all_meta.author = all_meta.author.apply(lambda x: x[0] if len(x) != 0 else '')\n",
    "all_meta.classification = all_meta.classification.apply(lambda x: x['lcc'][0] if 'lcc' in x else '')\n",
    "all_meta.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine a vocabulary\n",
    "\n",
    "Here, I count up all the words, by POS, and use that to trim the wordlist to interesting words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 11min 35.2s\n"
     ]
    }
   ],
   "source": [
    "# Multithread with dask\n",
    "def tokenlist_from_path(path):\n",
    "    try:\n",
    "        df = (FeatureReader(path)\n",
    "                 .first()\n",
    "                 .tokenlist(pages=False)\n",
    "                 .reset_index()\n",
    "                 .drop('section', 1))\n",
    "        assert df.columns.tolist() == ['token', 'pos', 'count']\n",
    "    except:\n",
    "        df = pd.DataFrame(columns=['token','pos','count'])\n",
    "    return df\n",
    "\n",
    "delayed_dfs = [delayed(tokenlist_from_path)(path) for path in paths]\n",
    "ddf = dd.from_delayed(delayed_dfs)\n",
    "\n",
    "with ProgressBar():\n",
    "    total_word_counts = ddf.groupby(['token', 'pos']).sum().compute().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>add</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>salt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>cook</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  token\n",
       "0   0    cup\n",
       "1   1    add\n",
       "2   2  water\n",
       "3   3   salt\n",
       "4   4   cook"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stoplist and lemmatize. This is mostly for the normalizing and filtering verbs,\n",
    "# but doesn't do anything regretful to the nouns\n",
    "def lemmatize(word):\n",
    "    return list(nlp(word))[0].lemma_\n",
    "\n",
    "# Keep nouns, verbs, and adjectives\n",
    "pos_filter = ['NNP', 'NN', 'NNS', 'NNPS'] + ['VB', 'VBN', 'VBD', 'VBZ', 'VBG', 'VBP'] + ['JJ', 'JJR', 'JJS']\n",
    "stoplist = total_word_counts.index.get_level_values('token').isin(spacy.en.STOPWORDS)\n",
    "\n",
    "pass1 = (total_word_counts[~stoplist]\n",
    "             .query('count > 20')\n",
    "             .loc[(slice(None), pos_filter),]\n",
    "             .reset_index()\n",
    "        )\n",
    "pass1['lemma'] = pass1.token.apply(lemmatize)\n",
    "\n",
    "pass2 = (pass1.query('lemma != \"\"')\n",
    "              .groupby('lemma', as_index=False)[['count']].sum()\n",
    "              .query('count > 100')\n",
    "        )\n",
    "\n",
    "# Final dictionary with keyed lemmas\n",
    "dictionary = pass2.sort_values('count', ascending=False).lemma.reset_index().rename(columns={'index':'id', 'lemma': 'token'})\n",
    "dictionary['id'] = range(0, len(dictionary))\n",
    "dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also key the volume ids\n",
    "volids = pd.Series(collection).reset_index().rename(columns={'index':'volid',0:'htid'})\n",
    "volids_dict = volids.set_index('htid').to_dict('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the processed info.\n",
    "with pd.HDFStore('cookbooks/ref.h5', mode='w', complib='blosc') as store:\n",
    "    store.append('metadata', all_meta)\n",
    "    store.append('global_frequencies', total_word_counts.query('count>10'))\n",
    "    store.append('dictionary', dictionary)\n",
    "    store.append('volids', volids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Page-level integer-keyed token counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>GROGAN</th>\n",
       "      <th>NNP</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>山己​シ</th>\n",
       "      <th>NN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Flueggea</th>\n",
       "      <th>NNP</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strawberry-nut</th>\n",
       "      <th>NNP</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ΕκφωιΜκο</th>\n",
       "      <th>NN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AÑÜS</th>\n",
       "      <th>NNP</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>左</th>\n",
       "      <th>NN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dressheat</th>\n",
       "      <th>NN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnﬁrns</th>\n",
       "      <th>NNS</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>​勢​</th>\n",
       "      <th>NN</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    count\n",
       "token          pos       \n",
       "GROGAN         NNP      2\n",
       "山己​シ           NN       1\n",
       "Flueggea       NNP      3\n",
       "Strawberry-nut NNP      1\n",
       "ΕκφωιΜκο       NN       1\n",
       "AÑÜS           NNP      1\n",
       "左              NN       1\n",
       "dressheat      NN       1\n",
       "cnﬁrns         NNS      1\n",
       "​勢​            NN       1"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = total_word_counts.sample(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('GROGAN', 'NNP'), ('山己\\u200bシ', 'NN'), ('Flueggea', 'NNP'),\n",
       "       ('Strawberry-nut', 'NNP'), ('ΕκφωιΜκο', 'NN'), ('AÑÜS', 'NNP'),\n",
       "       ('左', 'NN'), ('dressheat', 'NN'), ('cnﬁrns', 'NNS'),\n",
       "       ('\\u200b勢\\u200b', 'NN')], dtype=object)"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13529"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  0.6s\n",
      "[###################################     ] | 88% Completed |  1hr 14min  5.4s"
     ]
    }
   ],
   "source": [
    "def page_token_freq(path):\n",
    "    ''' Get token frequencies for words in the dictionary, per page and with volumes and tokens integer-keyed'''\n",
    "    try:\n",
    "        vol = FeatureReader(path).first()\n",
    "        tl = (vol.tokenlist()\n",
    "                 .loc[(slice(None), slice(None), slice(None), pos_filter),]\n",
    "                 .reset_index()\n",
    "                 .drop('section', 1)\n",
    "             )\n",
    "        assert tl.columns.tolist() == ['page', 'token', 'pos', 'count']\n",
    "        tl['lemma'] = tl.token.apply(lemmatize)\n",
    "        tl_folded = tl.groupby(['page', 'lemma'], as_index=False)[['count']].sum().rename(columns={'lemma':'token'})\n",
    "        tl_ids = pd.merge(tl_folded, dictionary)[['page', 'id', 'count']]\n",
    "        tl_ids['volid'] = volids_dict[vol.id]['volid']\n",
    "    except:\n",
    "        return pd.DataFrame(columns=['page', 'id', 'count', 'volid'])\n",
    "    return tl_ids\n",
    "\n",
    "with ProgressBar():\n",
    "    delayed_dfs = [delayed(page_token_freq)(path) for path in paths]\n",
    "    ddf = dd.from_delayed(delayed_dfs)\n",
    "    ddf.to_hdf('cookbooks/cookbook.*.h5', 'counts')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
